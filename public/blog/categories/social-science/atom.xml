<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: social science | Justin Murphy]]></title>
  <link href="http://jmrphy.github.io/blog/categories/social-science/atom.xml" rel="self"/>
  <link href="http://jmrphy.github.io/"/>
  <updated>2014-02-11T17:03:45+00:00</updated>
  <id>http://jmrphy.github.io/</id>
  <author>
    <name><![CDATA[Justin Murphy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A diagram of stratification]]></title>
    <link href="http://jmrphy.github.io/blog/2013/12/19/dg-stratifcation/"/>
    <updated>2013-12-19T06:53:00+00:00</updated>
    <id>http://jmrphy.github.io/blog/2013/12/19/dg-stratifcation</id>
    <content type="html"><![CDATA[<p>Below is a sketch of what Deleuze and Guattari describe as &ldquo;the pincer&rdquo; or &ldquo;double articulation&rdquo; of stratification.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<iframe src="http://wl.figshare.com/articles/881284/embed?show_title=0" width="650" height="424" frameborder="0"></iframe>


<p></p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Deleuze, Gilles, and Félix Guattari. 1987. A Thousand Plateaus. Minneapolis: University of Minnesota Press. pp. 40-41.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What really changed after the Cold War?]]></title>
    <link href="http://jmrphy.github.io/blog/2013/12/13/what-really-changed-after-the-cold-war/"/>
    <updated>2013-12-13T17:36:00+00:00</updated>
    <id>http://jmrphy.github.io/blog/2013/12/13/what-really-changed-after-the-cold-war</id>
    <content type="html"><![CDATA[<p>The traditional textbook story about the end of the Cold War and its effect on the international system completely misses the most important and extraordinary changes taking place at that time. The debate between realists and neoliberal institutionalists is a red herring. The most important transformation wrought by the collapse of the Soviet Union has nothing to do with the power of states vs. the power of international institutions. This debate in international relations theory completely distracts from what are very clearly the most world-historical changes taking place at that time, changes which actually have huge relevance to the lives of students and the global masses more generally.</p>

<p>In brief, the most important changes wrought by the collapse of the Soviet Union, in my view, are the following. These are not original insights, they have been well documented and highlighted by others but I bring them together here for my students and others who might be interested. I’m writing this off-the-cuff right after a seminar, but if I get a chance I’ll  come back and add some citations.</p>

<p>First, during the Cold War, there was a huge global alternative to capitalism (no matter how bad that alternative failed, the point is simply that capitalism was not the only game in town). Because of this, the U.S. and the other capitalist states had to at least pretend that they cared about people, they had to actually convince people that capitalism was a good system, they had to win the &ldquo;hearts and minds&rdquo; of people around the world.</p>

<p>What students today don&rsquo;t realise is that between the end of WWII and the collapse of the Soviet Union, poor people around the world were overthrowing their bosses, overthrowing their governments, and black and brown people of the global south were waging wars against their Western colonial rulers. These uprisings were NOT inherently authoritarian or Stalinesque projects: most of them were just radically democratic- and anti-capitalist uprisings where people were basically just saying &ldquo;take the land, resources, and political power back from the elites who have stolen from us; redistribute the land, cancel the debts, and let&rsquo;s make sure everyone has access to a decent life.&rdquo; And when these anti-colonial and essentially anti-capitalist uprisings would occur, the Soviet Union would give them money, recognition, and often implicit military backing. In this kind of global environment, the U.S. and other western capitalist states HAD to constrain their greed. Almost everything humane achieved within capitalist states since the end of WWII was in part because capitalist elites in the West were terrified that their own citizens would hang them and redistribute their wealth, power, and privileges.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> That was a real threat, and no matter how much we disagree with Stalin or Mao, they&rsquo;re global power gave huge inspiration and real military power to democratic and anti-capitalist uprisings (however badly they would also oppress these projects while supporting them).</p>

<p>So there&rsquo;s that: because the USSR was a threat, the capitalist elites of the West had to actually give their people some resources to at least make it look like capitalism was worth believing in. Or else the poor countries would go with the USSR and the US (and US capitalism) would be finished.</p>

<p>Second, though, is the other side of this coin. During the Cold War, the capitalist elites of the world had to worry about many things other than making money. Trade relations were obviously fraught around the world. Democratic uprisings in the global south were always threatening to expropriate foreign investment capital, whether it was explicitly colonial or not; the U.S. saw extraordinary wildcat strikes at the end of WWII; all kinds of anti-capitalist resistance made it very hard for the rich to get much richer! But it&rsquo;s not just that making money was inhibited by political antagonisms, it&rsquo;s that if anyone within a country got too rich, it would have instigated revolutionary uprisings to cut them down to size!</p>

<p>With the end of the Cold War and the collapse of a powerful global counterweight to the rule of capitalist elites, there was no longer anything to prevent the wealthiest and most powerful people in each country from becoming even wealthier and more powerful! Thus, one of the most extraordinary transformations at the end of the Cold War, which is often omitted or neglected from IR textbooks, is that formerly Communist elites quickly converted to capitalist elites in the single largest transfer of wealth the world has ever seen. In Russia and its Eastern European satellites, the whole economy which was previously owned by the state was simply handed over (often at extremely low prices) to Communist elites who would then simply become capitalist business people. The privatisation of state-owned enterprises after the collapse of the Soviet Union was the single largest transfer of wealth that&rsquo;s ever taken place in world history.</p>

<p>In countries such as Russia, China, India, and many smaller countries as well, the end of the Cold War meant the sudden appearance of the shadiest figure alive today: the billionaire. It is only after the collapse of the USSR, the disappearance of any moral, political, or military check on the power of national capitalist elites, that it is even conceivable for a country such as Mexico to generate one of the wealthiest billionaires in the world, Carlos Slim. Globalisation after the collapse of the USSR is simply the global unleashing of national capitalist elites, the complete global freedom of millionaires to become billionaires and to do whatever they want while politically crushing the masses of the world into more and more helpless, low-wage workers.</p>

<p>When an IR textbook asks us to debate whether states or markets are more powerful, we should be extremely skeptical. States and markets are quickly becoming the same thing, and the old debates of IR appear increasingly dubious. In my opinion, we should always follow the money: find out who is winning, who is losing, and formulate our own questions based on what we&rsquo;re actually observing. To do anything less runs the huge risk of being trapped in a perspective which is itself so narrow in part because of the very transformations we&rsquo;re only beginning to understand.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>The prime example in the UK is, of course, the National Health Service. Such broad-based social provision was a concession which the wealthy made to the masses because they were terrified that the British Communist Party would simply take what they saw as belonging to them. In the U.S., an interesting example is the limited success of the civil rights movement. So long as U.S. capitalism meant the domination and subjugation of its own racial minorities, that didn&rsquo;t look so good to the black and brown people of the world! If the U.S. was going to convince the black and brown people of the world to go the way of capitalism, it had to at least pretend that it supported the freedom of its own black people. Now that every single person born into the UK and US since 1985 is told from birth that there is no alternative to capitalism, is it any wonder that welfare spending is on the decline and the rich white people of the world don&rsquo;t give a shit about racism?<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A concept of political validity in quantitative social science]]></title>
    <link href="http://jmrphy.github.io/blog/2013/11/03/a-concept-of-political-validity-in-quantitative-social-science/"/>
    <updated>2013-11-03T17:12:00+00:00</updated>
    <id>http://jmrphy.github.io/blog/2013/11/03/a-concept-of-political-validity-in-quantitative-social-science</id>
    <content type="html"><![CDATA[<p>Let us call <em>bourgeois data</em> any research data the collection of which is funded by groups invested in the status quo. Bourgeois data, which is to say nearly all the quantitative data used by social scientists, suffer from a specific type of bias for which most methodologists show very little concern, despite the extraordinary analytical attention they devote to the question of bias in social-scientific inference more generally. Methodologists are likely unconcerned about the possibility of <em>institutional</em> biases in data collection because they have precisely the same institutional investments as the funders of data collection projects. Methodologists will consider &ldquo;external validity&rdquo; (whether the data reflects what we think it reflects in the world) and &ldquo;internal validity&rdquo; (the logical coherence of inferences), but rarely do they consider what we might call <em>political validity</em>, which we might provisionally define as the scientific legitimacy of the basic worldview, determined by a researcher’s institutional environment, with which scientific inferences are verbally constructed. In other words, we must take seriously the question of whether the institutional environment which sponsors data-collection and research activity more generally is itself non-random and non-trivial. I suspect that for too long this question has been delegated to “critical” theorists in a disingenuous gesture which gives them just enough attention so that positivist social scientists can say their discipline already has people “working on” this issue, but not so much attention that positivists have to make any serious adjustments in how we actually think, write, or speak about the political implications of our research agendas.</p>

<p>Unfortunately, bourgeois data are almost by definition generated by a non-random, non-trivial social process which biases at the very least its <em>terms</em>, if not necessarily the internal or external validity of conclusions drawn from it. By beginning with a vocabulary drawn from a status quo which is from a social-scientific perspective perfectly arbitrary, it ends with conclusions the <em>meaning of which</em> are equally arbitrary, no matter how internally or external valid those conclusions might be. More to the point, the non-random, non-trivial process which shapes a status quo into one vocabulary rather than another is, of course, power. This is why critical theory&rsquo;s demand for reflexivity in social science is not empty posturing, as so many positivist social scientists dismissively believe. Rather, that demand points to a bona fide problem of scientific inference which is not merely humanistic critique from cultural studies departments. To put the issue in the words of positivist social science: the problem is that the words we use to draw conclusions from empirical patterns are never &ldquo;controlled for.&rdquo; We need theoretical work which will, in a methodologically rigorous fashion, subject the naive vocabularies of contemporary social science to a verification process where arbitrary power-driven linguistic determinations are &ldquo;controlled for&rdquo; and replaced in those theoretical edifices we devise to “house” the empirical patterns we discover.</p>

<p>(Social scientists, it must be admitted, have a child-like trust of the politicians and institutions from which they inherit their vocabularies, perhaps because like children they live from the good graces of these parents.)</p>

<p>Most contemporary datasets respond to some research demand that is considered valuable from the perspective of the status quo; it will be motivated by hypotheses the confirmation and rejection of which are consistent with the status quo, for no other reason than all of the actors involved prefer the status quo to a radical overhaul. One doesn’t need to even be a Marxist to believe this, perfectly mainstream rational-choice theories as well as sociological institutionalism confirm this point easily. Because we know these simple truths about institutions but can’t be bothered to take them very seriously in our own work—because the committment to truth, to unbiased inference, would actually demand an explicitly revolutionary politics for which social science does not have the courage today, quantiative social scientists in particular are like finance columnists who evaluate businesses they own stock in without disclosing this fact to readers.</p>

<p>Fortunately, for those of us who take seriously the revolutionary political agenda implied by the necessity of the scientific method, it is not at all the case that we are simply awash with hopelessly and uselessly prejudiced data. Bourgeois data cannot help but accidentally shed light on aspects of the social system it never had any interest in and would have preferred to omit. And it is these aspects to which we must devote our attention if we are to glean from biased data the stories which are most important for correcting the data’s institutional biases. I will call <em>epiphanic data</em> those datasets which emerge from transforming, subsetting, modeling, and otherwise manipulating bourgeois data in order to glean from it radical, anti-institutional insights. It might be objected that if the institutions which sponsor data collection so contaminate data with political bias, then what allows us to think we can glean non-biased conclusions from it? In that voice which positivists use to insinuate that critical theorists are merely conspiracy theorists with paranoiac worldviews: “Why is it not the case that this data is doomed to reproduce the status quo, if the data-collecting instituions are so politically perverse?” There are a few reasons why even the most bourgeois data can be used for anti-bourgeis, epiphanic social science.</p>

<p>The drive for short-term profit incentivizes institutions and individuals to collect dangerously more data than is ideal for the perpetuation of the status quo, at the risk of revealing contradictions which indict the system as a whole. Those groups funding data collection in one area are often unaware and, in any event, politically unconcerned with data collected in another area. Of course, they are politically aware of issues such as general conflicts of bureaucratic “turf” but certainly not in the long-term, revolutionary political dimensions under consideration here. Bourgeois data collection is complacent also because the division of academic labor is so developed and intensely enforced that status quo institutions don’t really have to worry about anyone discovering relationships <em>across many interdisciplinary datasets</em>, even though such datasets are widely and freely available. Bourgeois institutions have been emboldened by a severe undersupply of revolutionaries in quantitative social science. This has permitted data-collectiong institutions a great deal of complacency on this point. A sudden exogenous increase in radical social scientists could quickly seize on bourgeois data and extract from it extremely damning indictments of the status quo, which from a technical perspective would be almost childishly simple but have never yet been performed because the institutional biases are such that these findings are not rewarded. At the same time, phenomena such as racism, colonialism and patriarchy have, for a very long time, artifically depressed literacy, education, and civil rights to specific groups, suppressing demand for a really true, popular social science which would dispel the most vicious falsities and generate more legitimate equilibria. So long as the less privileged groups could not effectivley demand true social science, bourgeois institutions felt safe to collect data to their cold heart’s content.</p>

<p>But today, decolonization, civil rights movements, feminist movements, various democratic uprisings, and the rise of mass education as well as the internet, have all lead to a huge increase in the aggregate demand for precisely this kind of social science. Witness the rise of the “data visualizer”, that not-quite-data-scientist internet persona who specializes in fashionable data-intensive infographics for mass distribution on the web. Such a persona is merely an inane example of how millions of people around the world would be increasingly amenable to a popular, emancipatory social science.</p>

<p>In summary, for those who wish to read between the lines of bourgeois data, there always exists epiphanic data in the simple will to read it differently, a parallax data which promises a wide world of revolutionary intellectual exploration. Unfortunately, this world of data is currently available only to those few of us who have been given the unequally distributed privileges required to mine it <em>and</em> a sufficiently youthful spirit to still <em>want to</em>. But a mass defection of radical young social scientists from currently privileged research agendas toward anti-institutional perspectives would open a large range of research which need not be technically sophisticated to quickly produce huge increases in both the overall truth quotient of social science and its overall effectiveness in radical egalitarian political change.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Civil war and the square of ethnic fractionalization]]></title>
    <link href="http://jmrphy.github.io/blog/2013/05/22/civil-war-and-the-square-of-ethnic-fractionalization-4/"/>
    <updated>2013-05-22T17:06:22+01:00</updated>
    <id>http://jmrphy.github.io/blog/2013/05/22/civil-war-and-the-square-of-ethnic-fractionalization-4</id>
    <content type="html"><![CDATA[<p>Steve Saideman wonders if the relationship between ethnic fractionalization and civil war is curvilinear, where increasing fractionalization increases the probability of civil war up to a certain point but then increasing fractionalization decreases the probability of civil war. Since I have my nose in this data right now, I&rsquo;ve given this conjecture a quick probe. I find no evidence <em>prima facie</em>.</p>

<p>Below is a replication of Sambanis 2004 where I simply add the square of ethnic fractionalization. If Steve&rsquo;s conjecture were true, we&rsquo;d expect <em>ef1</em> to be positive, <em>ef1sq</em> to be negative, and both of them to be significant. They are signed as expected but not significant given the controls recommended by Sambanis. I then try alternative codings of civil war and a simple equation with no controls. If Steve&rsquo;s conjecture is true, it&rsquo;s not obvious.</p>

<p>Everything below is reproducible in R-just download the replication data, easy to find with a quick search, and set your working directory to where the data is.</p>

<p>``` r Replicate Sambanis 2004 (Table 6 in paper, column 8, pp.845)
library(foreign)
sambanis&lt;-read.dta(&ldquo;SambanisJCR2004_replicationdataset.dta&rdquo;)
sambanis$ef1sq&lt;-sambanis$ef1 * sambanis$ef1</p>

<p>model &lt;&ndash; glm(warstnsb  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 +</p>

<pre><code>        ef1 + lpopnsl1 + mtnl1 + warnsl1, data = sambanis,
        family = binomial(link = "probit"))  
</code></pre>

<p>summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstnsb ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + 
##     oil2l1 + ef1 + lpopnsl1 + mtnl1 + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.604  -0.231  -0.173  -0.106   3.560  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.90226    0.46234   -8.44  &lt; 2e-16 ***
## gdpl1       -0.09271    0.02295   -4.04  5.3e-05 ***
## grol1       -0.51380    0.49750   -1.03  0.30172    
## inst3l1      0.23713    0.09634    2.46  0.01384 *  
## anoc2l1      0.23792    0.08807    2.70  0.00690 ** 
## oil2l1       0.29680    0.11541    2.57  0.01012 *  
## ef1          0.35605    0.16455    2.16  0.03049 *  
## lpopnsl1     0.10503    0.02743    3.83  0.00013 ***
## mtnl1        0.00199    0.00182    1.09  0.27466    
## warnsl1     -0.06609    0.10492   -0.63  0.52873    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1133.2  on 5892  degrees of freedom
## Residual deviance: 1038.8  on 5883  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 1059
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<p>``` r Adding the square of ethnic fractionalization
model &lt;&ndash; glm(warstnsb  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 +</p>

<pre><code>        ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, data = sambanis,
        family = binomial(link = "probit"))  
</code></pre>

<p>summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstnsb ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + 
##     oil2l1 + ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, 
##     family = binomial(link = "probit"), data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.595  -0.230  -0.172  -0.103   3.547  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.94606    0.48647   -8.11    5e-16 ***
## gdpl1       -0.09092    0.02370   -3.84  0.00012 ***
## grol1       -0.50874    0.49343   -1.03  0.30253    
## inst3l1      0.23723    0.09641    2.46  0.01386 *  
## anoc2l1      0.24253    0.08838    2.74  0.00607 ** 
## oil2l1       0.26000    0.12119    2.15  0.03192 *  
## ef1          0.51893    0.72622    0.71  0.47488    
## ef1sq       -0.17995    0.71442   -0.25  0.80113    
## lpopnsl1     0.10452    0.02775    3.77  0.00017 ***
## mtnl1        0.00184    0.00188    0.98  0.32529    
## muslim       0.00104    0.00111    0.93  0.35169    
## warnsl1     -0.06888    0.10501   -0.66  0.51185    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1133.2  on 5892  degrees of freedom
## Residual deviance: 1037.8  on 5881  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 1062
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<p>``` r Using Sambanis alternative coding of civil war
model &lt;&ndash; glm(warstns  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 +</p>

<pre><code>        ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1,
        data = sambanis, family = binomial(link = "probit"))  
</code></pre>

<p>summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstns ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + 
##     ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.532  -0.217  -0.165  -0.102   3.499  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.73245    0.52338   -7.13  9.9e-13 ***
## gdpl1       -0.08649    0.02469   -3.50  0.00046 ***
## grol1       -0.09783    0.48861   -0.20  0.84131    
## inst3l1      0.23635    0.10911    2.17  0.03030 *  
## anoc2l1      0.28191    0.09773    2.88  0.00392 ** 
## oil2l1       0.16875    0.14066    1.20  0.23027    
## ef1          0.65104    0.77115    0.84  0.39853    
## ef1sq       -0.38362    0.77341   -0.50  0.61989    
## lpopnsl1     0.08969    0.03039    2.95  0.00316 ** 
## mtnl1        0.00209    0.00205    1.02  0.30854    
## muslim       0.00118    0.00124    0.95  0.34246    
## warnsl1     -0.21649    0.31589   -0.69  0.49313    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 915.34  on 5161  degrees of freedom
## Residual deviance: 842.13  on 5150  degrees of freedom
##   (4298 observations deleted due to missingness)
## AIC: 866.1
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<p>``` r Using Fearon and Laitin 2003 coding of civil war
model &lt;&ndash; glm(warst7b  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 +</p>

<pre><code>        ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1,
        data = sambanis, family = binomial(link = "probit"))
</code></pre>

<p>summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warst7b ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + 
##     ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.501  -0.200  -0.145  -0.086   3.590  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -4.28989    0.54194   -7.92  2.5e-15 ***
## gdpl1       -0.08807    0.02665   -3.30  0.00095 ***
## grol1       -0.27141    0.52732   -0.51  0.60677    
## inst3l1      0.19536    0.10692    1.83  0.06768 .  
## anoc2l1      0.25845    0.09748    2.65  0.00802 ** 
## oil2l1       0.13327    0.14174    0.94  0.34707    
## ef1          0.44027    0.80427    0.55  0.58409    
## ef1sq       -0.22642    0.79784   -0.28  0.77657    
## lpopnsl1     0.11930    0.03065    3.89  9.9e-05 ***
## mtnl1        0.00255    0.00204    1.25  0.21124    
## muslim       0.00155    0.00122    1.26  0.20659    
## warnsl1      0.01688    0.11169    0.15  0.87987    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 897.81  on 5892  degrees of freedom
## Residual deviance: 822.48  on 5881  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 846.5
## 
## Number of Fisher Scoring iterations: 9
</code></pre>

<p><code>r Only ethnic fractionalization
model &lt;- glm(warstnsb  ef1 + ef1sq, data = sambanis, family = binomial(link = "probit"))
summary(model)
</code></p>

<pre><code>## Call:
## glm(formula = warstnsb ~ ef1 + ef1sq, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.303  -0.227  -0.196  -0.161   2.990  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -2.302      0.131  -17.63   &lt;2e-16 ***
## ef1            0.323      0.589    0.55     0.58    
## ef1sq          0.283      0.580    0.49     0.63    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1249.6  on 6268  degrees of freedom
## Residual deviance: 1230.5  on 6266  degrees of freedom
##   (3191 observations deleted due to missingness)
## AIC: 1237
## 
## Number of Fisher Scoring iterations: 7
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analysis of #GazaUnderAttack tweets]]></title>
    <link href="http://jmrphy.github.io/blog/2012/12/29/analysis-of-gazaunderattack-tweets/"/>
    <updated>2012-12-29T19:12:32+00:00</updated>
    <id>http://jmrphy.github.io/blog/2012/12/29/analysis-of-gazaunderattack-tweets</id>
    <content type="html"><![CDATA[<p>During the Israeli-Palestinian attacks of last month, I scraped from the Twitter API about 270,000 tweets containing the hashtag #gazaunderattack. Given the unprecedented degree to which this war was fought online, tweets of this sort could and should become really important data for political scientists. I&rsquo;m too busy right now to say very much here, but I want to share some basic descriptive analyses for anyone who might be interested. The dataset can be downloaded <a href="https://dl.dropbox.com/u/20498362/tweets_%23gazaunderattack.csv">here</a> and the R script that produced these analyses is available at the very bottom.</p>

<p>Almost everything in this post I learned how to do directly and solely from the amazingly smart, open and generous #Rstats community. The scraping and time-series plot follow a script by <a href="http://bommaritollc.com/2011/02/26/archiving-tweets-with-python/">Michael Bommarito</a>, and the rest follows scripts by <a href="https://github.com/benmarwick/AAA2011-Tweets/blob/master/AAA2011.R">Ben Marwick</a>.</p>

<h2>Analysis</h2>

<p>There are 269,158 tweets. These tweets are authored by 79,923 unique users. Of all the tweets, .62 are retweets.</p>

<p>This first graph plots the frequency of #gazaunderattack tweets in 30-minute intervals between November 17th and November 21st. I <em>believe</em> this is <em>all</em> of the tweets containing that hashtag within this period. I know the Twitter Search API is subject to weird filters and restrictions, but I believe the technique I used here pages through each and every tweet available within the available time period.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/timeseries.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/timeseries.png" alt="timeseries" /></a></p>

<p>Most frequent #gazaunderattack tweeters.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/tweet_counts.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/tweet_counts.png" alt="tweet_counts" /></a></p>

<p>The most retweeted tweeters. Interestingly, Anonymous seems to have had more reach, at least during this period, than the twitter account of Hamas (@AlqassamBrigade).</p>

<p><em><a href="http://justinmurphy.files.wordpress.com/2012/12/retweet_counts.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/retweet_counts.png" alt="retweet_counts" /></a></em>The most retweeted tweeters as a ratio of total quantity of tweets sent.   Anonymous still seems to have had the most reach on the #gazaunderattack hashtag.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/retweet_ratios.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/retweet_ratios.png" alt="retweet_ratios" /></a></p>

<p>Most frequently tweeted links.<a href="http://justinmurphy.files.wordpress.com/2012/12/links.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/links.png" alt="links" /></a></p>

<p><a href="http://d3j5vwomefv46c.cloudfront.net/photos/large/688740234.jpg?key=964633&amp;Expires=1356766099&amp;Key-Pair-Id=APKAIYVGSUJFNRFZBBTA&amp;Signature=05VhXXvHpkOc2wqjLXrGMgNasVet~TM9zp9UELk3vd0aCJcGb6uJI4Uv4FEk5LNEQQSGWrUrV9mNKpp5STIrUEwFufBGCwcboTeLJfg55DA75JoXHkMdmedD5P6M2~EOYUbtqSOBFGY7VQgzfN-~UhU6lLSwV3grA4~ZrZDTIlI_">This is the image</a> in the most popular link, capturing an explosion from an Israeli airstrike in Gaza.</p>

<p>Again, the Python code I used to obtain the tweets and the R code I used to analyze them were lifted directly from scripts by the authors linked above.</p>

<p>``` r Analyze Tweets scraped with Python</p>

<p>x&lt;-read.csv(&ldquo;tweets_#gazaunderattack.csv&rdquo;, header=FALSE, stringsAsFactors=FALSE)
x$username&lt;-x$V2
x$text&lt;-x$V5</p>

<h6>#</h6>

<h4>Nice Time-Series Plot</h4>

<h6>#</h6>

<p>library(ggplot2)
x$date &lt;&ndash; strptime(x$V4, &ldquo;%a, %d %b %Y %H:%M:%S %z&rdquo;, tz = &ldquo;EST&rdquo;)
x$date &lt;&ndash; as.POSIXct(x$date, tz = &ldquo;EST&rdquo;)
timeseries&lt;-ggplot(data=x, aes(x=date)) + geom_bar(aes(fill=..count..), binwidth=60*30) + theme_bw() + ylab(&ldquo;# of Tweets&rdquo;) + xlab(&ldquo;Time&rdquo;)
timeseries
ggsave(file=&ldquo;timeseries.png&rdquo;)</p>

<h6>#</h6>

<h4>Nice Plot of Frequent Tweeters</h4>

<h6>#</h6>

<p>library(ggplot2)
x$username[x$username==&ldquo;&rdquo;]&lt;-NA
length(unique(x$username)) # see how many unique tweeter accounts in the sample
counts=table(x$username)
counts.sort&lt;-sort(counts)
counts.sort.subset=subset(counts.sort, counts.sort>350) # create a subset of those who tweeted at least 350 times or more
counts.sort.subset.df&lt;-data.frame(people=unlist(dimnames(counts.sort.subset)),count=unlist(counts.sort.subset)) # makes a funny sort of data frame&hellip;
counts.sort.subset.df&lt;-data.frame(people=as.factor(counts.sort.subset.df$people),counts=as.numeric(counts.sort.subset.df$count)) # makes a better data frame for ggplot to work with
ggplot(counts.sort.subset.df, aes(reorder(people,counts),counts)) + xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Number of messages&rdquo;)+ geom_bar() + coord_flip() + theme_bw() + opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) + opts(axis.title.y=theme_text(size = 14, angle=90)) # plot nicely ordered counts of tweets by person for people > 5 tggsave(file = &ldquo;tweet_counts.pdf&rdquo;) # export the plot to a PDF file
ggsave(file = &ldquo;tweet_counts.png&rdquo;)</p>

<h6>#</h6>

<h4>Nice Plot of Frequent Re-Tweeters</h4>

<h6>#</h6>

<p>library(stringr)
x$text=sapply(x$text,function(row) iconv(row,to=&lsquo;UTF-8&rsquo;)) #remove odd characters
trim &lt;&ndash; function (x) sub(&lsquo;@&rsquo;,&lsquo;&rsquo;,x) # remove @ symbol from user names
x$rt=sapply(x$text,function(tweet) trim(str_match(tweet,&ldquo;^RT (@[[:alnum:]_]*)&rdquo;)<a href="http://bommaritollc.com/2011/02/26/archiving-tweets-with-python/">2</a>)) #extract who has been RT’d
sum(!is.na(x$rt)) # see how many tweets are retweets
sum(!is.na(x$rt))/length(x$rt) # the ratio of retweets to tweets
countRT&lt;-table(x$rt)
countRT&lt;-sort(countRT)
countRT.subset=subset(countRT,countRT>1000) # subset those RT’d more than 1000 times
countRT.subset.df&lt;-data.frame(people=as.factor(unlist(dimnames(countRT.subset))),RT_count=as.numeric(unlist(countRT.subset)))
ggplot(countRT.subset.df, aes(reorder(people,RT_count),RT_count)) +
 xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Number of messages retweeted by others&rdquo;) +
 geom_bar() + coord_flip() + theme_bw() +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))
 # plot nicely ordered counts of tweets by person for people > 1000 retweets
ggsave(file = &ldquo;retweet_counts.png&rdquo;)</p>

<h6>#</h6>

<h4>Nice Plot of RT-Tweet Ratios</h4>

<h6>#</h6>

<p>t&lt;-as.data.frame(table(x$username)) # make table with counts of tweets per person
rt&lt;-as.data.frame(table(x$rt)) # make table with counts of retweets per person
t.rt&lt;-merge(t,rt,by=&ldquo;Var1&rdquo;) # combine tweet count and retweet count per person
t.rt[&ldquo;ratio&rdquo;]&lt;-t.rt$Freq.y / t.rt$Freq.x # creates new col and adds ratio tweet/retweet
sort.t.rt&lt;-t.rt[order(t.rt$ratio),] # sort it to put names in order by ratio
sort.t.rt.subset&lt;-subset(sort.t.rt,sort.t.rt$Freq.y>1000) # exclude those with 1000 tweets or less
sort.t.rt.subset.drop&lt;-droplevels(sort.t.rt.subset) # drop unused levels that got in there somehow&hellip; note that this is already a data frame
ggplot(sort.t.rt.subset, aes(reorder(Var1,ratio),ratio)) +
 xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Retweets as a ratio of total tweets&rdquo;) +
 geom_bar() + coord_flip() + theme_bw() +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))
ggsave(file = &ldquo;retweet_ratios.png&rdquo;)</p>

<h6>#</h6>

<h4>Nice Plot of Most Popular Links</h4>

<h6>#</h6>

<p>x$link=sapply(x$text,function(tweet) str_extract(tweet,(&ldquo;http[[:print:]]+&rdquo;))) # creates new field and extracts the links contained in the tweet
x$link=sapply(x$text,function(tweet) str_extract(tweet,&ldquo;http[[:print:]]{16}&rdquo;)) # limits to just 16 characters after http so I just get the shortened link. They are all shortened, so this is fine, but there might be a better way using regex.
countlink&lt;-table(x$link) # get frequencies of each link
countlink&lt;-sort(countlink) # sort them
barplot(countlink) # plot freqs</p>

<h1>or to use ggplot2, read on&hellip;</h1>

<p>countlink&lt;-data.frame(table(na.omit((x$link))))
countlink&lt;-subset(countlink,countlink$Freq>300) # exclude those with 300 tweets or less
ggplot(countlink, aes(reorder(Var1, Freq), Freq)) +
 geom_bar() + coord_flip() + theme_bw() +
 xlab(&ldquo;Link&rdquo;) + ylab(&ldquo;Frequency&rdquo;) +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))
ggsave(file = &ldquo;links.png&rdquo;)</p>

<p>```</p>
]]></content>
  </entry>
  
</feed>
