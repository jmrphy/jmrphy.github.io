<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: social science | Justin Murphy]]></title>
  <link href="http://jmrphy.github.io/blog/categories/social-science/atom.xml" rel="self"/>
  <link href="http://jmrphy.github.io/"/>
  <updated>2013-10-12T23:43:46+01:00</updated>
  <id>http://jmrphy.github.io/</id>
  <author>
    <name><![CDATA[Justin Murphy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Civil war and the square of ethnic fractionalization]]></title>
    <link href="http://jmrphy.github.io/blog/2013/05/22/civil-war-and-the-square-of-ethnic-fractionalization-4/"/>
    <updated>2013-05-22T17:06:22+01:00</updated>
    <id>http://jmrphy.github.io/blog/2013/05/22/civil-war-and-the-square-of-ethnic-fractionalization-4</id>
    <content type="html"><![CDATA[<p>Steve Saideman wonders if the relationship between ethnic fractionalization and civil war is curvilinear, where increasing fractionalization increases the probability of civil war up to a certain point but then increasing fractionalization decreases the probability of civil war. Since I have my nose in this data right now, I&rsquo;ve given this conjecture a quick probe. I find no evidence <em>prima facie</em>.</p>

<p>Below is a replication of Sambanis 2004 where I simply add the square of ethnic fractionalization. If Steve&rsquo;s conjecture were true, we&rsquo;d expect <em>ef1</em> to be positive, <em>ef1sq</em> to be negative, and both of them to be significant. They are signed as expected but not significant given the controls recommended by Sambanis. I then try alternative codings of civil war and a simple equation with no controls. If Steve&rsquo;s conjecture is true, it&rsquo;s not obvious.</p>

<p>Everything below is reproducible in R-just download the replication data, easy to find with a quick search, and set your working directory to where the data is.</p>

<h3>Get data and create the squared variable</h3>

<p>```
library(foreign)
sambanis &lt;&ndash; read.dta(&ldquo;SambanisJCR2004_replicationdataset.dta&rdquo;)
sambanis$ef1sq &lt;&ndash; sambanis$ef1 * sambanis$ef1
```</p>

<h3>Replicate Sambanis 2004 (Table 6 in paper, column 8, pp.845)</h3>

<p>```
model &lt;&ndash; glm(warstnsb  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 + lpopnsl1 + mtnl1 + warnsl1, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))</p>

<p>summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstnsb ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + 
##     oil2l1 + ef1 + lpopnsl1 + mtnl1 + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.604  -0.231  -0.173  -0.106   3.560  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.90226    0.46234   -8.44  &lt; 2e-16 ***
## gdpl1       -0.09271    0.02295   -4.04  5.3e-05 ***
## grol1       -0.51380    0.49750   -1.03  0.30172    
## inst3l1      0.23713    0.09634    2.46  0.01384 *  
## anoc2l1      0.23792    0.08807    2.70  0.00690 ** 
## oil2l1       0.29680    0.11541    2.57  0.01012 *  
## ef1          0.35605    0.16455    2.16  0.03049 *  
## lpopnsl1     0.10503    0.02743    3.83  0.00013 ***
## mtnl1        0.00199    0.00182    1.09  0.27466    
## warnsl1     -0.06609    0.10492   -0.63  0.52873    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1133.2  on 5892  degrees of freedom
## Residual deviance: 1038.8  on 5883  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 1059
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<h3>Adding the square of ethnic fractionalization</h3>

<p>```
model &lt;&ndash; glm(warstnsb  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))<br/>
summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstnsb ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + 
##     oil2l1 + ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, 
##     family = binomial(link = "probit"), data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.595  -0.230  -0.172  -0.103   3.547  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.94606    0.48647   -8.11    5e-16 ***
## gdpl1       -0.09092    0.02370   -3.84  0.00012 ***
## grol1       -0.50874    0.49343   -1.03  0.30253    
## inst3l1      0.23723    0.09641    2.46  0.01386 *  
## anoc2l1      0.24253    0.08838    2.74  0.00607 ** 
## oil2l1       0.26000    0.12119    2.15  0.03192 *  
## ef1          0.51893    0.72622    0.71  0.47488    
## ef1sq       -0.17995    0.71442   -0.25  0.80113    
## lpopnsl1     0.10452    0.02775    3.77  0.00017 ***
## mtnl1        0.00184    0.00188    0.98  0.32529    
## muslim       0.00104    0.00111    0.93  0.35169    
## warnsl1     -0.06888    0.10501   -0.66  0.51185    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1133.2  on 5892  degrees of freedom
## Residual deviance: 1037.8  on 5881  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 1062
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<h3>Using Sambanis alternative coding of civil war</h3>

<p>```
model &lt;&ndash; glm(warstns  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))<br/>
summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstns ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + 
##     ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.532  -0.217  -0.165  -0.102   3.499  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.73245    0.52338   -7.13  9.9e-13 ***
## gdpl1       -0.08649    0.02469   -3.50  0.00046 ***
## grol1       -0.09783    0.48861   -0.20  0.84131    
## inst3l1      0.23635    0.10911    2.17  0.03030 *  
## anoc2l1      0.28191    0.09773    2.88  0.00392 ** 
## oil2l1       0.16875    0.14066    1.20  0.23027    
## ef1          0.65104    0.77115    0.84  0.39853    
## ef1sq       -0.38362    0.77341   -0.50  0.61989    
## lpopnsl1     0.08969    0.03039    2.95  0.00316 ** 
## mtnl1        0.00209    0.00205    1.02  0.30854    
## muslim       0.00118    0.00124    0.95  0.34246    
## warnsl1     -0.21649    0.31589   -0.69  0.49313    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 915.34  on 5161  degrees of freedom
## Residual deviance: 842.13  on 5150  degrees of freedom
##   (4298 observations deleted due to missingness)
## AIC: 866.1
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<h3>Using Fearon and Laitin 2003 coding of civil war</h3>

<p>```
model &lt;&ndash; glm(warst7b  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 + ef1sq +
lpopnsl1 + mtnl1 + muslim + warnsl1, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))
summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warst7b ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + 
##     ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.501  -0.200  -0.145  -0.086   3.590  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -4.28989    0.54194   -7.92  2.5e-15 ***
## gdpl1       -0.08807    0.02665   -3.30  0.00095 ***
## grol1       -0.27141    0.52732   -0.51  0.60677    
## inst3l1      0.19536    0.10692    1.83  0.06768 .  
## anoc2l1      0.25845    0.09748    2.65  0.00802 ** 
## oil2l1       0.13327    0.14174    0.94  0.34707    
## ef1          0.44027    0.80427    0.55  0.58409    
## ef1sq       -0.22642    0.79784   -0.28  0.77657    
## lpopnsl1     0.11930    0.03065    3.89  9.9e-05 ***
## mtnl1        0.00255    0.00204    1.25  0.21124    
## muslim       0.00155    0.00122    1.26  0.20659    
## warnsl1      0.01688    0.11169    0.15  0.87987    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 897.81  on 5892  degrees of freedom
## Residual deviance: 822.48  on 5881  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 846.5
## 
## Number of Fisher Scoring iterations: 9
</code></pre>

<h3>Only ethnic fractionalization</h3>

<p>```
model &lt;&ndash; glm(warstnsb  ef1 + ef1sq, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))</p>

<p>summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstnsb ~ ef1 + ef1sq, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.303  -0.227  -0.196  -0.161   2.990  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -2.302      0.131  -17.63   &lt;2e-16 ***
## ef1            0.323      0.589    0.55     0.58    
## ef1sq          0.283      0.580    0.49     0.63    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1249.6  on 6268  degrees of freedom
## Residual deviance: 1230.5  on 6266  degrees of freedom
##   (3191 observations deleted due to missingness)
## AIC: 1237
## 
## Number of Fisher Scoring iterations: 7
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analysis of #GazaUnderAttack tweets]]></title>
    <link href="http://jmrphy.github.io/blog/2012/12/29/analysis-of-gazaunderattack-tweets/"/>
    <updated>2012-12-29T19:12:32+00:00</updated>
    <id>http://jmrphy.github.io/blog/2012/12/29/analysis-of-gazaunderattack-tweets</id>
    <content type="html"><![CDATA[<p>During the Israeli-Palestinian attacks of last month, I scraped from the Twitter API about 270,000 tweets containing the hashtag #gazaunderattack. Given the unprecedented degree to which this war was fought online, tweets of this sort could and should become really important data for political scientists. I&rsquo;m too busy right now to say very much here, but I want to share some basic descriptive analyses for anyone who might be interested. The dataset can be downloaded <a href="https://dl.dropbox.com/u/20498362/tweets_%23gazaunderattack.csv">here</a> and the R script that produced these analyses is available at the very bottom.</p>

<p>Almost everything in this post I learned how to do directly and solely from the amazingly smart, open and generous #Rstats community. The scraping and time-series plot follow a script by <a href="http://bommaritollc.com/2011/02/26/archiving-tweets-with-python/">Michael Bommarito</a>, and the rest follows scripts by <a href="https://github.com/benmarwick/AAA2011-Tweets/blob/master/AAA2011.R">Ben Marwick</a>.</p>

<h2>Analysis</h2>

<p>There are 269,158 tweets. These tweets are authored by 79,923 unique users. Of all the tweets, .62 are retweets.</p>

<p>This first graph plots the frequency of #gazaunderattack tweets in 30-minute intervals between November 17th and November 21st. I _believe__ _this is <em>all</em> of the tweets containing that hashtag within this period. I know the Twitter Search API is subject to weird filters and restrictions, but I believe the technique I used here pages through each and every tweet available within the available time period.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/timeseries.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/timeseries.png" alt="timeseries" /></a></p>

<p>Most frequent #gazaunderattack tweeters.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/tweet_counts.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/tweet_counts.png" alt="tweet_counts" /></a></p>

<p>The most retweeted tweeters. Interestingly, Anonymous seems to have had more reach, at least during this period, than the twitter account of Hamas (@AlqassamBrigade).</p>

<p><em><a href="http://justinmurphy.files.wordpress.com/2012/12/retweet_counts.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/retweet_counts.png" alt="retweet_counts" /></a></em>The most retweeted tweeters as a ratio of total quantity of tweets sent.   Anonymous still seems to have had the most reach on the #gazaunderattack hashtag.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/retweet_ratios.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/retweet_ratios.png" alt="retweet_ratios" /></a></p>

<p>Most frequently tweeted links.<a href="http://justinmurphy.files.wordpress.com/2012/12/links.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/links.png" alt="links" /></a></p>

<p><a href="http://d3j5vwomefv46c.cloudfront.net/photos/large/688740234.jpg?key=964633&amp;Expires=1356766099&amp;Key-Pair-Id=APKAIYVGSUJFNRFZBBTA&amp;Signature=05VhXXvHpkOc2wqjLXrGMgNasVet~TM9zp9UELk3vd0aCJcGb6uJI4Uv4FEk5LNEQQSGWrUrV9mNKpp5STIrUEwFufBGCwcboTeLJfg55DA75JoXHkMdmedD5P6M2~EOYUbtqSOBFGY7VQgzfN-~UhU6lLSwV3grA4~ZrZDTIlI_">This is the image</a> in the most popular link, capturing an explosion from an Israeli airstrike in Gaza.</p>

<p>Again, the Python code I used to obtain the tweets and the R code I used to analyze them were lifted directly from scripts by the authors linked above.</p>

<p>` r
`
x&lt;-read.csv(&ldquo;tweets_#gazaunderattack.csv&rdquo;, header=FALSE, stringsAsFactors=FALSE)
x$username&lt;-x$V2
x$text&lt;-x$V5</p>

<h6>##################################\</h6>

<h4>Nice Time-Series Plot</h4>

<h6>##################################\</h6>

<p>library(ggplot2)
x$date &lt;&ndash; strptime(x$V4, &ldquo;%a, %d %b %Y %H:%M:%S %z&rdquo;, tz = &ldquo;EST&rdquo;)
x$date &lt;&ndash; as.POSIXct(x$date, tz = &ldquo;EST&rdquo;)
timeseries&lt;-ggplot(data=x, aes(x=date)) + geom_bar(aes(fill=..count..), binwidth=60*30) + theme_bw() + ylab(&ldquo;# of Tweets&rdquo;) + xlab(&ldquo;Time&rdquo;)
timeseries
ggsave(file=&ldquo;timeseries.png&rdquo;)</p>

<h6>##################################\</h6>

<h4>Nice Plot of Frequent Tweeters</h4>

<h6>##################################\</h6>

<p>library(ggplot2)
x$username<a href="">x$username==&ldquo;&rdquo;</a>&lt;-NA
length(unique(x$username)) # see how many unique tweeter accounts in the sample
counts=table(x$username)
counts.sort&lt;-sort(counts)
counts.sort.subset=subset(counts.sort, counts.sort>350) # create a subset of those who tweeted at least 350 times or more
counts.sort.subset.df&lt;-data.frame(people=unlist(dimnames(counts.sort.subset)),count=unlist(counts.sort.subset)) # makes a funny sort of data frame&hellip;
counts.sort.subset.df&lt;-data.frame(people=as.factor(counts.sort.subset.df$people),counts=as.numeric(counts.sort.subset.df$count)) # makes a better data frame for ggplot to work with
ggplot(counts.sort.subset.df, aes(reorder(people,counts),counts)) + xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Number of messages&rdquo;)+ geom_bar() + coord_flip() + theme_bw() + opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) + opts(axis.title.y=theme_text(size = 14, angle=90)) # plot nicely ordered counts of tweets by person for people > 5 tggsave(file = &ldquo;tweet_counts.pdf&rdquo;) # export the plot to a PDF file
ggsave(file = &ldquo;tweet_counts.png&rdquo;)</p>

<h6>####################################\</h6>

<h4>Nice Plot of Frequent Re-Tweeters</h4>

<h6>####################################\</h6>

<p>library(stringr)
x$text=sapply(x$text,function(row) iconv(row,to=&lsquo;UTF-8&rsquo;)) #remove odd characters
trim &lt;&ndash; function (x) sub(&lsquo;@&rsquo;,&lsquo;&rsquo;,x) # remove @ symbol from user names
x$rt=sapply(x$text,function(tweet) trim(str_match(tweet,&ldquo;^RT (@[[:alnum:]_]()*)&rdquo;)<a href="">2</a>)) #extract who has been RT’d
sum(!is.na(x$rt)) # see how many tweets are retweets
sum(!is.na(x$rt))/length(x$rt) # the ratio of retweets to tweets
countRT&lt;-table(x$rt)
countRT&lt;-sort(countRT)
countRT.subset=subset(countRT,countRT>1000) # subset those RT’d more than 1000 times
countRT.subset.df&lt;-data.frame(people=as.factor(unlist(dimnames(countRT.subset))),RT_count=as.numeric(unlist(countRT.subset)))
ggplot(countRT.subset.df, aes(reorder(people,RT_count),RT_count)) +
 xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Number of messages retweeted by others&rdquo;) +
 geom_bar() + coord_flip() + theme_bw() +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))</p>

<h1>plot nicely ordered counts of tweets by person for people > 1000 retweets</h1>

<p>ggsave(file = &ldquo;retweet_counts.png&rdquo;)</p>

<h6>####################################\</h6>

<h4>Nice Plot of RT-Tweet Ratios</h4>

<h6>####################################\</h6>

<p>t&lt;-as.data.frame(table(x$username)) # make table with counts of tweets per person
rt&lt;-as.data.frame(table(x$rt)) # make table with counts of retweets per person
t.rt&lt;-merge(t,rt,by=&ldquo;Var1&rdquo;) # combine tweet count and retweet count per person
t.rt<a href="">&ldquo;ratio&rdquo;</a>&lt;-t.rt$Freq.y / t.rt$Freq.x # creates new col and adds ratio tweet/retweet
sort.t.rt&lt;-t.rt<a href="">order(t.rt$ratio),</a> # sort it to put names in order by ratio
sort.t.rt.subset&lt;-subset(sort.t.rt,sort.t.rt$Freq.y>1000) # exclude those with 1000 tweets or less
sort.t.rt.subset.drop&lt;-droplevels(sort.t.rt.subset) # drop unused levels that got in there somehow&hellip; note that this is already a data frame
ggplot(sort.t.rt.subset, aes(reorder(Var1,ratio),ratio)) +
 xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Retweets as a ratio of total tweets&rdquo;) +
 geom_bar() + coord_flip() + theme_bw() +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))
ggsave(file = &ldquo;retweet_ratios.png&rdquo;)</p>

<h6>####################################\</h6>

<h4>Nice Plot of Most Popular Links</h4>

<h6>####################################\</h6>

<p>x$link=sapply(x$text,function(tweet) str_extract(tweet,(&ldquo;http[[:print:]]()+&rdquo;))) # creates new field and extracts the links contained in the tweet
x$link=sapply(x$text,function(tweet) str_extract(tweet,&ldquo;http[[:print:]](){16}&rdquo;)) # limits to just 16 characters after http so I just get the shortened link. They are all shortened, so this is fine, but there might be a better way using regex.
countlink&lt;-table(x$link) # get frequencies of each link
countlink&lt;-sort(countlink) # sort them
barplot(countlink) # plot freqs</p>

<h1>or to use ggplot2, read on&hellip;</h1>

<p>countlink&lt;-data.frame(table(na.omit((x$link))))
countlink&lt;-subset(countlink,countlink$Freq>300) # exclude those with 300 tweets or less
ggplot(countlink, aes(reorder(Var1, Freq), Freq)) +
 geom_bar() + coord_flip() + theme_bw() +
 xlab(&ldquo;Link&rdquo;) + ylab(&ldquo;Frequency&rdquo;) +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))
ggsave(file = &ldquo;links.png&rdquo;)</p>

<p>``</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Predicting turnout from Google search data]]></title>
    <link href="http://jmrphy.github.io/blog/2012/10/27/predicting-turnout-from-google-search-data/"/>
    <updated>2012-10-27T22:58:43+01:00</updated>
    <id>http://jmrphy.github.io/blog/2012/10/27/predicting-turnout-from-google-search-data</id>
    <content type="html"><![CDATA[<p>Political scientists <a href="http://themonkeycage.org/blog/2012/10/25/will-2012-be-a-low-turnout-election-some-evidence-from-google-search-activity/%23comments">Jesse Richman</a> and <a href="http://themonkeycage.org/blog/2012/10/25/google-searches-and-turnout-a-cautionary-tale/">Erik Voeten</a> wonder if Google Search data can help us predict turnout in next month&rsquo;s Presidential election. To summarize, Richman finds that searches for &ldquo;vote&rdquo; in the past two elections predict voter turnout, and that low search volume for &ldquo;vote&rdquo; this year suggests a low turnout election. In response, Voeten shows that controlling for who has come online since 2004 and 2008 explains much of why search volume for &ldquo;vote&rdquo; appears small this year. There are lots of reasons search data is pretty tricky for making good inferences, but that shouldn&rsquo;t keep us from trying!</p>

<p>My thinking is that we can look at the _composition_ of &ldquo;vote&rdquo; searches across elections to see whether this year&rsquo;s vote searches are indicative of greater voter turnout or greater voter demobilization/skepticism.</p>

<p>For instance, &ldquo;vote&rdquo; searches containing the terms &ldquo;where&rdquo; or &ldquo;when&rdquo; are probably more directly predictive of turning out to vote than the whole set of &ldquo;vote&rdquo; searches&mdash;as they indicate practical planning to turnout. On the other hand, &ldquo;vote&rdquo; searches containing the term &ldquo;why&rdquo; are probably far less predictive of turnout&mdash;and might even predict how many people are thinking &ldquo;why bother?&rdquo; and are considering staying home on election day.</p>

<p>So I went back to the data with this in mind. As baseline and as Voeten shows, if we consider voting searches (&ldquo;vote&rdquo; and &ldquo;voting&rdquo; together) as a proportion of searches for &ldquo;university&rdquo;, &ldquo;science&rdquo;, and &ldquo;law&rdquo;, this election hardly looks very different. So that&rsquo;s just (vote+voting) / ((university+science+law) / 3). As explained in the posts linked above, search volume data is scaled from 0 to 100 where 100 is the peak of that search term in the given time period, relative to all other search activity.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/10/votingsearches2.png"><img src="http://justinmurphy.files.wordpress.com/2012/10/votingsearches2.png" alt="" /></a>And when we look at voting searches also containing the word &ldquo;why&rdquo; (as a proportion of the university-science-law index), again we find almost no difference.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/10/whyvotesearches.png"><img src="http://justinmurphy.files.wordpress.com/2012/10/whyvotesearches.png" alt="" /></a>But when we look at voting searches also containing the words &ldquo;where&rdquo; and &ldquo;when&rdquo; (I take the average of vote+where and vote+when as a proportion of the university-science-law index), more people appear to be asking where and when to vote this year than in the past two Presidential elections. The p-value on the difference of means between 2008 and 2012 is .051.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/10/votingplansearches.png"><img src="http://justinmurphy.files.wordpress.com/2012/10/votingplansearches.png" alt="" /></a>Finally, I look at the difference between the why-vote searches and the where/when-vote searches. The gap might be taken as an aggregate measure of how much society is using the web to <em>think about_ the decision to turn out </em>without planning_ to turnout, so that positive values might capture demobilization within vote searches (why bother?) and negative values reflect relatively resolved mobilization. And what we find is that this whole election year shows greater &ldquo;when/where&rdquo; vote searches than &ldquo;why&rdquo; vote searches at the present moment in the timeline but also for the year as a whole. In both of the previous elections there were more &ldquo;thinking&rdquo; searches than &ldquo;planning&rdquo; searches, but this year there have been more planning searches. As in the previous analyses, these are as a proportion of the university-science-law index, and the difference of means for 2008 and 2012 is statistically significant at the 99.9% confidence level.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/10/thinkplangapusl.png"><img src="http://justinmurphy.files.wordpress.com/2012/10/thinkplangapusl.png" alt="" /></a>Arguably, this difference variable perhaps already controls for education or political information because each term reflects a lack of knowledge, so it shouldn&rsquo;t be inflated by less educated/informed people coming online since 2004 and 2008. But the same pattern is evident and statistically significant if we look at this gap as a proportion of all &ldquo;politics&rdquo; search volume or just the raw differences of volume. Obviously, these are back-of-the napkin analyses and these data are plagued by difficulties. But these analyses don&rsquo;t suggest a low-turnout election in 2012. If anything, it looks like the web is registering more vote planning than in previous years.</p>

<p><a href="https://dl.dropbox.com/u/20498362/GoogleTrendsVoting.csv">Here is the data in .csv form</a>. Here is the <a href="https://dl.dropbox.com/u/20498362/GoogleTrendsTurnout.R">R script</a> to reproduce the analyses found here.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Does Obama really care less than Bush about climate change?]]></title>
    <link href="http://jmrphy.github.io/blog/2011/01/30/3018373241/"/>
    <updated>2011-01-30T21:35:00+00:00</updated>
    <id>http://jmrphy.github.io/blog/2011/01/30/3018373241</id>
    <content type="html"><![CDATA[<p>Suzanne Goldenberg of the <em>Guardian</em> reports a graph, apparently by <a href="http://www.bris.ac.uk/spais/people/person/148552">Matthew Hope</a> of the University of Bristol, to illustrate the claim that Obama is <a href="http://www.guardian.co.uk/environment/2011/jan/26/climate-change-obama-bush">less interested in climate change than Bush</a>. This comes on the heels of several critics who point out that Obama did not mention climate change once in his past State of the Union address. The graph displays aggregate mentions of &ldquo;climate change,&rdquo; &ldquo;environnment,&rdquo; and &ldquo;global warming.&rdquo;</p>

<p><a href="http://www.guardian.co.uk/environment/2011/jan/26/climate-change-obama-bush"><img src="http://media.tumblr.com/tumblr_lfutfsoliB1qz9517.jpg" alt="" /></a></p>

<p>However, as Ben Jervey of <em>Good</em> points out, Goldenberg&rsquo;s inference from her analysis <a href="http://www.guardian.co.uk/environment/2011/jan/26/climate-change-obama-bush">is wrong</a>. As Jervey suggests, Obama&rsquo;s lack of interest in &ldquo;climate change&rdquo; is due simply to a shift in rhetorical strategy, as Obama has chosen to frame the problems of climate change as an issue of &ldquo;clean energy.&rdquo; An analysis similar to Goldenberg&rsquo;s provides some compelling evidence for the latter view. Below, I&rsquo;m posting a graph comparable to Goldenberg&rsquo;s but with mentions of &ldquo;energy&rdquo; rather than &ldquo;climate change,&rdquo; &ldquo;environment,&rdquo; and &ldquo;global warming.&rdquo; As graphics should, this largely speaks for itself:<img src="http://media.tumblr.com/tumblr_lfuudeGs1v1qz9517.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
