<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: social science | Justin Murphy]]></title>
  <link href="http://jmrphy.github.io/blog/categories/social-science/atom.xml" rel="self"/>
  <link href="http://jmrphy.github.io/"/>
  <updated>2013-10-12T23:52:13+01:00</updated>
  <id>http://jmrphy.github.io/</id>
  <author>
    <name><![CDATA[Justin Murphy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Civil war and the square of ethnic fractionalization]]></title>
    <link href="http://jmrphy.github.io/blog/2013/05/22/civil-war-and-the-square-of-ethnic-fractionalization-4/"/>
    <updated>2013-05-22T17:06:22+01:00</updated>
    <id>http://jmrphy.github.io/blog/2013/05/22/civil-war-and-the-square-of-ethnic-fractionalization-4</id>
    <content type="html"><![CDATA[<p>Steve Saideman wonders if the relationship between ethnic fractionalization and civil war is curvilinear, where increasing fractionalization increases the probability of civil war up to a certain point but then increasing fractionalization decreases the probability of civil war. Since I have my nose in this data right now, I&rsquo;ve given this conjecture a quick probe. I find no evidence <em>prima facie</em>.</p>

<p>Below is a replication of Sambanis 2004 where I simply add the square of ethnic fractionalization. If Steve&rsquo;s conjecture were true, we&rsquo;d expect <em>ef1</em> to be positive, <em>ef1sq</em> to be negative, and both of them to be significant. They are signed as expected but not significant given the controls recommended by Sambanis. I then try alternative codings of civil war and a simple equation with no controls. If Steve&rsquo;s conjecture is true, it&rsquo;s not obvious.</p>

<p>Everything below is reproducible in R-just download the replication data, easy to find with a quick search, and set your working directory to where the data is.</p>

<h3>Get data and create the squared variable</h3>

<p>```
library(foreign)
sambanis &lt;&ndash; read.dta(&ldquo;SambanisJCR2004_replicationdataset.dta&rdquo;)
sambanis$ef1sq &lt;&ndash; sambanis$ef1 * sambanis$ef1
```</p>

<h3>Replicate Sambanis 2004 (Table 6 in paper, column 8, pp.845)</h3>

<p>```
model &lt;&ndash; glm(warstnsb  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 + lpopnsl1 + mtnl1 + warnsl1, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))</p>

<p>summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstnsb ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + 
##     oil2l1 + ef1 + lpopnsl1 + mtnl1 + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.604  -0.231  -0.173  -0.106   3.560  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.90226    0.46234   -8.44  &lt; 2e-16 ***
## gdpl1       -0.09271    0.02295   -4.04  5.3e-05 ***
## grol1       -0.51380    0.49750   -1.03  0.30172    
## inst3l1      0.23713    0.09634    2.46  0.01384 *  
## anoc2l1      0.23792    0.08807    2.70  0.00690 ** 
## oil2l1       0.29680    0.11541    2.57  0.01012 *  
## ef1          0.35605    0.16455    2.16  0.03049 *  
## lpopnsl1     0.10503    0.02743    3.83  0.00013 ***
## mtnl1        0.00199    0.00182    1.09  0.27466    
## warnsl1     -0.06609    0.10492   -0.63  0.52873    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1133.2  on 5892  degrees of freedom
## Residual deviance: 1038.8  on 5883  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 1059
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<h3>Adding the square of ethnic fractionalization</h3>

<p>```
model &lt;&ndash; glm(warstnsb  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))<br/>
summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstnsb ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + 
##     oil2l1 + ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, 
##     family = binomial(link = "probit"), data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.595  -0.230  -0.172  -0.103   3.547  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.94606    0.48647   -8.11    5e-16 ***
## gdpl1       -0.09092    0.02370   -3.84  0.00012 ***
## grol1       -0.50874    0.49343   -1.03  0.30253    
## inst3l1      0.23723    0.09641    2.46  0.01386 *  
## anoc2l1      0.24253    0.08838    2.74  0.00607 ** 
## oil2l1       0.26000    0.12119    2.15  0.03192 *  
## ef1          0.51893    0.72622    0.71  0.47488    
## ef1sq       -0.17995    0.71442   -0.25  0.80113    
## lpopnsl1     0.10452    0.02775    3.77  0.00017 ***
## mtnl1        0.00184    0.00188    0.98  0.32529    
## muslim       0.00104    0.00111    0.93  0.35169    
## warnsl1     -0.06888    0.10501   -0.66  0.51185    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1133.2  on 5892  degrees of freedom
## Residual deviance: 1037.8  on 5881  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 1062
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<h3>Using Sambanis alternative coding of civil war</h3>

<p>```
model &lt;&ndash; glm(warstns  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))<br/>
summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstns ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + 
##     ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.532  -0.217  -0.165  -0.102   3.499  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.73245    0.52338   -7.13  9.9e-13 ***
## gdpl1       -0.08649    0.02469   -3.50  0.00046 ***
## grol1       -0.09783    0.48861   -0.20  0.84131    
## inst3l1      0.23635    0.10911    2.17  0.03030 *  
## anoc2l1      0.28191    0.09773    2.88  0.00392 ** 
## oil2l1       0.16875    0.14066    1.20  0.23027    
## ef1          0.65104    0.77115    0.84  0.39853    
## ef1sq       -0.38362    0.77341   -0.50  0.61989    
## lpopnsl1     0.08969    0.03039    2.95  0.00316 ** 
## mtnl1        0.00209    0.00205    1.02  0.30854    
## muslim       0.00118    0.00124    0.95  0.34246    
## warnsl1     -0.21649    0.31589   -0.69  0.49313    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 915.34  on 5161  degrees of freedom
## Residual deviance: 842.13  on 5150  degrees of freedom
##   (4298 observations deleted due to missingness)
## AIC: 866.1
## 
## Number of Fisher Scoring iterations: 8
</code></pre>

<h3>Using Fearon and Laitin 2003 coding of civil war</h3>

<p>```
model &lt;&ndash; glm(warst7b  gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + ef1 + ef1sq +
lpopnsl1 + mtnl1 + muslim + warnsl1, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))
summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warst7b ~ gdpl1 + grol1 + inst3l1 + anoc2l1 + oil2l1 + 
##     ef1 + ef1sq + lpopnsl1 + mtnl1 + muslim + warnsl1, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.501  -0.200  -0.145  -0.086   3.590  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -4.28989    0.54194   -7.92  2.5e-15 ***
## gdpl1       -0.08807    0.02665   -3.30  0.00095 ***
## grol1       -0.27141    0.52732   -0.51  0.60677    
## inst3l1      0.19536    0.10692    1.83  0.06768 .  
## anoc2l1      0.25845    0.09748    2.65  0.00802 ** 
## oil2l1       0.13327    0.14174    0.94  0.34707    
## ef1          0.44027    0.80427    0.55  0.58409    
## ef1sq       -0.22642    0.79784   -0.28  0.77657    
## lpopnsl1     0.11930    0.03065    3.89  9.9e-05 ***
## mtnl1        0.00255    0.00204    1.25  0.21124    
## muslim       0.00155    0.00122    1.26  0.20659    
## warnsl1      0.01688    0.11169    0.15  0.87987    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 897.81  on 5892  degrees of freedom
## Residual deviance: 822.48  on 5881  degrees of freedom
##   (3567 observations deleted due to missingness)
## AIC: 846.5
## 
## Number of Fisher Scoring iterations: 9
</code></pre>

<h3>Only ethnic fractionalization</h3>

<p>```
model &lt;&ndash; glm(warstnsb  ef1 + ef1sq, data = sambanis, family = binomial(link = &ldquo;probit&rdquo;))</p>

<p>summary(model)
```</p>

<pre><code>## Call:
## glm(formula = warstnsb ~ ef1 + ef1sq, family = binomial(link = "probit"), 
##     data = sambanis)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.303  -0.227  -0.196  -0.161   2.990  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -2.302      0.131  -17.63   &lt;2e-16 ***
## ef1            0.323      0.589    0.55     0.58    
## ef1sq          0.283      0.580    0.49     0.63    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1249.6  on 6268  degrees of freedom
## Residual deviance: 1230.5  on 6266  degrees of freedom
##   (3191 observations deleted due to missingness)
## AIC: 1237
## 
## Number of Fisher Scoring iterations: 7
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analysis of #GazaUnderAttack tweets]]></title>
    <link href="http://jmrphy.github.io/blog/2012/12/29/analysis-of-gazaunderattack-tweets/"/>
    <updated>2012-12-29T19:12:32+00:00</updated>
    <id>http://jmrphy.github.io/blog/2012/12/29/analysis-of-gazaunderattack-tweets</id>
    <content type="html"><![CDATA[<p>During the Israeli-Palestinian attacks of last month, I scraped from the Twitter API about 270,000 tweets containing the hashtag #gazaunderattack. Given the unprecedented degree to which this war was fought online, tweets of this sort could and should become really important data for political scientists. I&rsquo;m too busy right now to say very much here, but I want to share some basic descriptive analyses for anyone who might be interested. The dataset can be downloaded <a href="https://dl.dropbox.com/u/20498362/tweets_%23gazaunderattack.csv">here</a> and the R script that produced these analyses is available at the very bottom.</p>

<p>Almost everything in this post I learned how to do directly and solely from the amazingly smart, open and generous #Rstats community. The scraping and time-series plot follow a script by <a href="http://bommaritollc.com/2011/02/26/archiving-tweets-with-python/">Michael Bommarito</a>, and the rest follows scripts by <a href="https://github.com/benmarwick/AAA2011-Tweets/blob/master/AAA2011.R">Ben Marwick</a>.</p>

<h2>Analysis</h2>

<p>There are 269,158 tweets. These tweets are authored by 79,923 unique users. Of all the tweets, .62 are retweets.</p>

<p>This first graph plots the frequency of #gazaunderattack tweets in 30-minute intervals between November 17th and November 21st. I _believe__ _this is <em>all</em> of the tweets containing that hashtag within this period. I know the Twitter Search API is subject to weird filters and restrictions, but I believe the technique I used here pages through each and every tweet available within the available time period.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/timeseries.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/timeseries.png" alt="timeseries" /></a></p>

<p>Most frequent #gazaunderattack tweeters.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/tweet_counts.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/tweet_counts.png" alt="tweet_counts" /></a></p>

<p>The most retweeted tweeters. Interestingly, Anonymous seems to have had more reach, at least during this period, than the twitter account of Hamas (@AlqassamBrigade).</p>

<p><em><a href="http://justinmurphy.files.wordpress.com/2012/12/retweet_counts.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/retweet_counts.png" alt="retweet_counts" /></a></em>The most retweeted tweeters as a ratio of total quantity of tweets sent.   Anonymous still seems to have had the most reach on the #gazaunderattack hashtag.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/12/retweet_ratios.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/retweet_ratios.png" alt="retweet_ratios" /></a></p>

<p>Most frequently tweeted links.<a href="http://justinmurphy.files.wordpress.com/2012/12/links.png"><img src="http://justinmurphy.files.wordpress.com/2012/12/links.png" alt="links" /></a></p>

<p><a href="http://d3j5vwomefv46c.cloudfront.net/photos/large/688740234.jpg?key=964633&amp;Expires=1356766099&amp;Key-Pair-Id=APKAIYVGSUJFNRFZBBTA&amp;Signature=05VhXXvHpkOc2wqjLXrGMgNasVet~TM9zp9UELk3vd0aCJcGb6uJI4Uv4FEk5LNEQQSGWrUrV9mNKpp5STIrUEwFufBGCwcboTeLJfg55DA75JoXHkMdmedD5P6M2~EOYUbtqSOBFGY7VQgzfN-~UhU6lLSwV3grA4~ZrZDTIlI_">This is the image</a> in the most popular link, capturing an explosion from an Israeli airstrike in Gaza.</p>

<p>Again, the Python code I used to obtain the tweets and the R code I used to analyze them were lifted directly from scripts by the authors linked above.</p>

<p>` r
`
x&lt;-read.csv(&ldquo;tweets_#gazaunderattack.csv&rdquo;, header=FALSE, stringsAsFactors=FALSE)
x$username&lt;-x$V2
x$text&lt;-x$V5</p>

<h6>##################################\</h6>

<h4>Nice Time-Series Plot</h4>

<h6>##################################\</h6>

<p>library(ggplot2)
x$date &lt;&ndash; strptime(x$V4, &ldquo;%a, %d %b %Y %H:%M:%S %z&rdquo;, tz = &ldquo;EST&rdquo;)
x$date &lt;&ndash; as.POSIXct(x$date, tz = &ldquo;EST&rdquo;)
timeseries&lt;-ggplot(data=x, aes(x=date)) + geom_bar(aes(fill=..count..), binwidth=60*30) + theme_bw() + ylab(&ldquo;# of Tweets&rdquo;) + xlab(&ldquo;Time&rdquo;)
timeseries
ggsave(file=&ldquo;timeseries.png&rdquo;)</p>

<h6>##################################\</h6>

<h4>Nice Plot of Frequent Tweeters</h4>

<h6>##################################\</h6>

<p>library(ggplot2)
x$username<a href="">x$username==&ldquo;&rdquo;</a>&lt;-NA
length(unique(x$username)) # see how many unique tweeter accounts in the sample
counts=table(x$username)
counts.sort&lt;-sort(counts)
counts.sort.subset=subset(counts.sort, counts.sort>350) # create a subset of those who tweeted at least 350 times or more
counts.sort.subset.df&lt;-data.frame(people=unlist(dimnames(counts.sort.subset)),count=unlist(counts.sort.subset)) # makes a funny sort of data frame&hellip;
counts.sort.subset.df&lt;-data.frame(people=as.factor(counts.sort.subset.df$people),counts=as.numeric(counts.sort.subset.df$count)) # makes a better data frame for ggplot to work with
ggplot(counts.sort.subset.df, aes(reorder(people,counts),counts)) + xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Number of messages&rdquo;)+ geom_bar() + coord_flip() + theme_bw() + opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) + opts(axis.title.y=theme_text(size = 14, angle=90)) # plot nicely ordered counts of tweets by person for people > 5 tggsave(file = &ldquo;tweet_counts.pdf&rdquo;) # export the plot to a PDF file
ggsave(file = &ldquo;tweet_counts.png&rdquo;)</p>

<h6>####################################\</h6>

<h4>Nice Plot of Frequent Re-Tweeters</h4>

<h6>####################################\</h6>

<p>library(stringr)
x$text=sapply(x$text,function(row) iconv(row,to=&lsquo;UTF-8&rsquo;)) #remove odd characters
trim &lt;&ndash; function (x) sub(&lsquo;@&rsquo;,&lsquo;&rsquo;,x) # remove @ symbol from user names
x$rt=sapply(x$text,function(tweet) trim(str_match(tweet,&ldquo;^RT (@[[:alnum:]_]()*)&rdquo;)<a href="">2</a>)) #extract who has been RT’d
sum(!is.na(x$rt)) # see how many tweets are retweets
sum(!is.na(x$rt))/length(x$rt) # the ratio of retweets to tweets
countRT&lt;-table(x$rt)
countRT&lt;-sort(countRT)
countRT.subset=subset(countRT,countRT>1000) # subset those RT’d more than 1000 times
countRT.subset.df&lt;-data.frame(people=as.factor(unlist(dimnames(countRT.subset))),RT_count=as.numeric(unlist(countRT.subset)))
ggplot(countRT.subset.df, aes(reorder(people,RT_count),RT_count)) +
 xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Number of messages retweeted by others&rdquo;) +
 geom_bar() + coord_flip() + theme_bw() +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))</p>

<h1>plot nicely ordered counts of tweets by person for people > 1000 retweets</h1>

<p>ggsave(file = &ldquo;retweet_counts.png&rdquo;)</p>

<h6>####################################\</h6>

<h4>Nice Plot of RT-Tweet Ratios</h4>

<h6>####################################\</h6>

<p>t&lt;-as.data.frame(table(x$username)) # make table with counts of tweets per person
rt&lt;-as.data.frame(table(x$rt)) # make table with counts of retweets per person
t.rt&lt;-merge(t,rt,by=&ldquo;Var1&rdquo;) # combine tweet count and retweet count per person
t.rt<a href="">&ldquo;ratio&rdquo;</a>&lt;-t.rt$Freq.y / t.rt$Freq.x # creates new col and adds ratio tweet/retweet
sort.t.rt&lt;-t.rt<a href="">order(t.rt$ratio),</a> # sort it to put names in order by ratio
sort.t.rt.subset&lt;-subset(sort.t.rt,sort.t.rt$Freq.y>1000) # exclude those with 1000 tweets or less
sort.t.rt.subset.drop&lt;-droplevels(sort.t.rt.subset) # drop unused levels that got in there somehow&hellip; note that this is already a data frame
ggplot(sort.t.rt.subset, aes(reorder(Var1,ratio),ratio)) +
 xlab(&ldquo;Author&rdquo;) + ylab(&ldquo;Retweets as a ratio of total tweets&rdquo;) +
 geom_bar() + coord_flip() + theme_bw() +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))
ggsave(file = &ldquo;retweet_ratios.png&rdquo;)</p>

<h6>####################################\</h6>

<h4>Nice Plot of Most Popular Links</h4>

<h6>####################################\</h6>

<p>x$link=sapply(x$text,function(tweet) str_extract(tweet,(&ldquo;http[[:print:]]()+&rdquo;))) # creates new field and extracts the links contained in the tweet
x$link=sapply(x$text,function(tweet) str_extract(tweet,&ldquo;http[[:print:]](){16}&rdquo;)) # limits to just 16 characters after http so I just get the shortened link. They are all shortened, so this is fine, but there might be a better way using regex.
countlink&lt;-table(x$link) # get frequencies of each link
countlink&lt;-sort(countlink) # sort them
barplot(countlink) # plot freqs</p>

<h1>or to use ggplot2, read on&hellip;</h1>

<p>countlink&lt;-data.frame(table(na.omit((x$link))))
countlink&lt;-subset(countlink,countlink$Freq>300) # exclude those with 300 tweets or less
ggplot(countlink, aes(reorder(Var1, Freq), Freq)) +
 geom_bar() + coord_flip() + theme_bw() +
 xlab(&ldquo;Link&rdquo;) + ylab(&ldquo;Frequency&rdquo;) +
 opts(axis.title.x = theme_text(vjust = -0.5, size = 14)) +
 opts(axis.title.y=theme_text(size = 14, angle=90))
ggsave(file = &ldquo;links.png&rdquo;)</p>

<p>``</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Predicting turnout from Google search data]]></title>
    <link href="http://jmrphy.github.io/blog/2012/10/27/predicting-turnout-from-google-search-data/"/>
    <updated>2012-10-27T22:58:43+01:00</updated>
    <id>http://jmrphy.github.io/blog/2012/10/27/predicting-turnout-from-google-search-data</id>
    <content type="html"><![CDATA[<p>Political scientists <a href="http://themonkeycage.org/blog/2012/10/25/will-2012-be-a-low-turnout-election-some-evidence-from-google-search-activity/%23comments">Jesse Richman</a> and <a href="http://themonkeycage.org/blog/2012/10/25/google-searches-and-turnout-a-cautionary-tale/">Erik Voeten</a> wonder if Google Search data can help us predict turnout in next month&rsquo;s Presidential election. To summarize, Richman finds that searches for &ldquo;vote&rdquo; in the past two elections predict voter turnout, and that low search volume for &ldquo;vote&rdquo; this year suggests a low turnout election. In response, Voeten shows that controlling for who has come online since 2004 and 2008 explains much of why search volume for &ldquo;vote&rdquo; appears small this year. There are lots of reasons search data is pretty tricky for making good inferences, but that shouldn&rsquo;t keep us from trying!</p>

<p>My thinking is that we can look at the _composition_ of &ldquo;vote&rdquo; searches across elections to see whether this year&rsquo;s vote searches are indicative of greater voter turnout or greater voter demobilization/skepticism.</p>

<p>For instance, &ldquo;vote&rdquo; searches containing the terms &ldquo;where&rdquo; or &ldquo;when&rdquo; are probably more directly predictive of turning out to vote than the whole set of &ldquo;vote&rdquo; searches&mdash;as they indicate practical planning to turnout. On the other hand, &ldquo;vote&rdquo; searches containing the term &ldquo;why&rdquo; are probably far less predictive of turnout&mdash;and might even predict how many people are thinking &ldquo;why bother?&rdquo; and are considering staying home on election day.</p>

<p>So I went back to the data with this in mind. As baseline and as Voeten shows, if we consider voting searches (&ldquo;vote&rdquo; and &ldquo;voting&rdquo; together) as a proportion of searches for &ldquo;university&rdquo;, &ldquo;science&rdquo;, and &ldquo;law&rdquo;, this election hardly looks very different. So that&rsquo;s just (vote+voting) / ((university+science+law) / 3). As explained in the posts linked above, search volume data is scaled from 0 to 100 where 100 is the peak of that search term in the given time period, relative to all other search activity.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/10/votingsearches2.png"><img src="http://justinmurphy.files.wordpress.com/2012/10/votingsearches2.png" alt="" /></a>And when we look at voting searches also containing the word &ldquo;why&rdquo; (as a proportion of the university-science-law index), again we find almost no difference.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/10/whyvotesearches.png"><img src="http://justinmurphy.files.wordpress.com/2012/10/whyvotesearches.png" alt="" /></a>But when we look at voting searches also containing the words &ldquo;where&rdquo; and &ldquo;when&rdquo; (I take the average of vote+where and vote+when as a proportion of the university-science-law index), more people appear to be asking where and when to vote this year than in the past two Presidential elections. The p-value on the difference of means between 2008 and 2012 is .051.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/10/votingplansearches.png"><img src="http://justinmurphy.files.wordpress.com/2012/10/votingplansearches.png" alt="" /></a>Finally, I look at the difference between the why-vote searches and the where/when-vote searches. The gap might be taken as an aggregate measure of how much society is using the web to <em>think about_ the decision to turn out </em>without planning_ to turnout, so that positive values might capture demobilization within vote searches (why bother?) and negative values reflect relatively resolved mobilization. And what we find is that this whole election year shows greater &ldquo;when/where&rdquo; vote searches than &ldquo;why&rdquo; vote searches at the present moment in the timeline but also for the year as a whole. In both of the previous elections there were more &ldquo;thinking&rdquo; searches than &ldquo;planning&rdquo; searches, but this year there have been more planning searches. As in the previous analyses, these are as a proportion of the university-science-law index, and the difference of means for 2008 and 2012 is statistically significant at the 99.9% confidence level.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/10/thinkplangapusl.png"><img src="http://justinmurphy.files.wordpress.com/2012/10/thinkplangapusl.png" alt="" /></a>Arguably, this difference variable perhaps already controls for education or political information because each term reflects a lack of knowledge, so it shouldn&rsquo;t be inflated by less educated/informed people coming online since 2004 and 2008. But the same pattern is evident and statistically significant if we look at this gap as a proportion of all &ldquo;politics&rdquo; search volume or just the raw differences of volume. Obviously, these are back-of-the napkin analyses and these data are plagued by difficulties. But these analyses don&rsquo;t suggest a low-turnout election in 2012. If anything, it looks like the web is registering more vote planning than in previous years.</p>

<p><a href="https://dl.dropbox.com/u/20498362/GoogleTrendsVoting.csv">Here is the data in .csv form</a>. Here is the <a href="https://dl.dropbox.com/u/20498362/GoogleTrendsTurnout.R">R script</a> to reproduce the analyses found here.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Comparing the twitter verbosity of @BarackObama and @MittRomney]]></title>
    <link href="http://jmrphy.github.io/blog/2012/05/22/comparing-the-twitter-verbosity-of-barackobama-and-mittromney-16/"/>
    <updated>2012-05-22T10:45:18+01:00</updated>
    <id>http://jmrphy.github.io/blog/2012/05/22/comparing-the-twitter-verbosity-of-barackobama-and-mittromney-16</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve decided to start learning how to scrape/analyze data from the Twitter API. Starting with pretty simple stuff, I used the TwitteR package in R to scrape almost all of the tweets by both U.S.presidential nominees. It&rsquo;s only possible to download 3200 tweets at a time, but this accounts for most of Obama&rsquo;s tweets (4,016 at the time of writing this) and all of Romney&rsquo;s (953).</p>

<p>One question we can ask is whether either nominee tweets lengthier tweets than the other. We know that G.W. Bush cultivated a populist sort of anti-intellectual style of speaking, we think of McCain&rsquo;s &ldquo;Straight Talk Express&rdquo; bus tour, and of course Obama is often characterized as an academic elite.</p>

<p><a href="http://justinmurphy.files.wordpress.com/2012/05/rplot3.jpg"><img src="http://justinmurphy.files.wordpress.com/2012/05/rplot3.jpg?w=470" alt="Image" /></a></p>

<p>Interestingly, although the modal length of tweets for both candidates is around the maximum of 140, @MittRomney spikes a little bit around 50 characters. It&rsquo;s impossible to say from this little graphic whether this difference reveals something systematic, but maybe the spike of roughly 50-character tweets from @MittRomney reflects a more &ldquo;straight talking&rdquo; tweeter than @BarackObama?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[U.S. media and ideological filtering as a global "public good"]]></title>
    <link href="http://jmrphy.github.io/blog/2011/12/20/14486977984/"/>
    <updated>2011-12-20T01:56:00+00:00</updated>
    <id>http://jmrphy.github.io/blog/2011/12/20/14486977984</id>
    <content type="html"><![CDATA[<p>In the study of international relations, there is a conventional and well-demonstrated theory according to which 1.) global stability is to be understood as a “public good,” and 2.) that due to strategic problems obstructing collective cooperation, often a hegemon is required to effectively subsidize it (Krasner 1976; Kindleberger 1981). Put very simply, in the absence of a hegemon, the global system tends toward conflict in a &ldquo;race to the bottom.&rdquo; Many states with more or less equal power and acting in their own self-interest calculate that, for instance, raising tariffs will help their domestic industries. However, the outcome is that as every other state acts similarly, all countries fare worse than they would have had they not raised tariffs. A global hegemon can prevent this bad equilibrium by effectively enforcing global free trade. More specifically, it is commonly argued that these conditions characterized the global system immediately after World War II, with the United States subsidizing the construction of an “embedded liberalism” (Ruggie 1983) and using “strategic restraint” (Ikenberry 2001) to favor global free trade rather than traditional militaristic domination.</p>

<p>We can develop and further specifiy this line of thought by elaborating how one of the “public goods” subsidized by liberal American hegemony was perhaps the maintenance of a certain ideological hygiene for the postwar capitalist order. One could argue that U.S. culture industry, nurtured by a media market that exemplifies the liberal, commercial model of state-media relations as well as a liberal bourgeois public opinion, has functioned as a sort of protective filter in global media circulation, a vacuum drain that shoots into the dumpster of forgotten history any ugly flotsam that rises up from the placid sea of global capitalist ideology. </p>

<p>This filter for the global desiring machine performs the specific and essential function of making any threat to the system replaced by whatever U.S. media puts in its place. The US media, in this model, is a sort of condom for catching the ejaculant of global capitalism, preventing the excesses of capitalist desire from fertilizing offspring that will ultimately contest the father.</p>

<p>More recently, variation across the covers of Time Magazine’s different regional editions provide stunning evidence for this claim. Below are the various regional editions of Time Magazine for the weeks of October 24th and December 5th of this writing (2011).</p>

<p><img src="http://media.tumblr.com/tumblr_lwhb4v7QFj1qz9517.png" alt="" />
<img src="http://media.tumblr.com/tumblr_lwhb0p8eU01qz9517.png" alt="" /></p>

<p>Whereas the rest of the world sees a proud Egyptian revolutionary in a gas mask, readers in the United States see a story not just about anxiety, but why it&rsquo;s &ldquo;good for you.&rdquo; These images, only suggestive from a social scientific perspective, provide almost too-provocative evidence of the hypothesis advanced in the last section, that differences in U.S. news reportage perform a certain function of ideological filtering for global flows of desire. </p>

<p> Although we remain for the moment agnostic with respect to the specific characteristics of the U.S. media that lead it to function as global ideological filter, the hypothesis along with the patently conservative biases exemplified in these magazine covers would be consistent with the dyanmic patterns of variation <a href="http://Justin%20Murphyields.tumblr.com/post/14483036324/a-return-of-the-repressed-preliminary-theory-and">uncovered previously using data from Google Books</a>. Material political or economic shocks to the system (radical rebellions such as that of May 1968 in France or the Egyptian uprising this past year) enters into regional or global news but is prevented from causing truly global disruptions of consciousness by a domestic U.S. media quickly replacing it with the sterile human interest stories for which it is famous. Because the U.S. is one of the chief exporters of culture in the world, this sterilization of news consumption for even just the U.S. domestic market represents an <em>energy sink</em> for the whole global system. If revolutionary energy doesn’t get in, it obviously doesn’t get sent back out, and this is especially significant because so much of global culture is set by the American culture industries.</p>
]]></content>
  </entry>
  
</feed>
